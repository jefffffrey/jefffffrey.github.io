{"meta":{"title":"Jeffrey's Blog","subtitle":"Live and learn!","description":null,"author":"Jeffrey Teo","url":"http://yoursite.com"},"pages":[{"title":"about","date":"2017-10-28T10:43:33.000Z","updated":"2017-10-28T10:43:33.252Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""}],"posts":[{"title":"Caddy proxy指令详解","slug":"Caddy proxy指令详解","date":"2018-03-05T08:40:12.000Z","updated":"2018-03-05T08:44:39.325Z","comments":true,"path":"2018/03/05/Caddy proxy指令详解/","link":"","permalink":"http://yoursite.com/2018/03/05/Caddy proxy指令详解/","excerpt":"","text":"简介Caddy的proxy可以用于配置: 普通的反向代理 负载均衡:支持多策略,健康检查,故障转移 WebSocket代理 开启该指令后,Caddy的proxy中间件会添加一个可用于{upstream}的占位符,用于记录哪一个代理的处理的请求. proxy完整语法格式为: proxy from to... &#123; policy name [value] fail_timeout duration max_fails integer max_conns integer try_duration duration try_interval duration health_check path health_check_port port health_check_interval interval_duration health_check_timeout timeout_duration header_upstream name value header_downstream name value keepalive number without prefix except ignored_paths... upstream to insecure_skip_verify preset&#125; from 被代理的请求的路径 to 代理的断点,可以是多个,支持 http/https/quic/srv/unix/quic, 如果是srv://或者srv+https://则被认为是一个服务定位器,Caddy将通过SRV DNS去解析 policy 负载均衡策略,必须有多个后端.如果后端是服务定位器则不可以,可选的有random, least_conn, round_robin, first, ip_hash, uri_hash 和 header, 默认为random fail_timeout 失败了的请求记住多久.如果在这期间达到了max_fails,则被考虑为挂掉了,不会再有请求发送过去,除非失败的请求被忘记了.默认为0,总是可用. max_fails 如果fail_timeout为0则无效,默认为1 max_conns 每一个后端的最大请求数,默认0,没有限制,如果达到了返回502错误. try_duration 第一个上游失败了,挂起外面来的请求多久 try_interval 某一个上游服务器挂了之后等待多久切换下一个上有服务器,默认250ms. 设置为0并且设置了duration可能占满cpu health_check 设置一个路径,Caddy将定期发出请求,如果返回200和399就认为正常,否则就不正常,然后提前定义的interval间隔里他就是被考虑不正常的. health_check_port 设置一个额外的检查端口,如果是服务定位器就无效 health_check_interval 健康检查间隔,默认30s health_check_timeout 检查超时时间,默认60s header_upstream 传递到后方的头,可以多个,也可以用请求占位符 header_downstream 修改返回的响应头 keepalive 保持多少和后端之间闲置的连接,设置为0关闭keepalive without 去掉的前缀(/api/foo without /api 将访问 /foo. ) except设置不代理的路径 upstream 基本等同于to配置 insecure_skip_verify 重载后端TLS认证,禁用HTTPS的安全功能 preset 支持的有websocket和transparent Presetswebsocket等价于: header_upstream Connection &#123;&gt;Connection&#125;header_upstream Upgrade &#123;&gt;Upgrade&#125; transparent等价于: header_upstream Host &#123;host&#125;header_upstream X-Real-IP &#123;remote&#125;header_upstream X-Forwarded-For &#123;remote&#125;header_upstream X-Forwarded-Proto &#123;scheme&#125; 负载均衡策略 random (default) - 随机选择 least_conn - 选择活跃连接最少的 round_robin - 循环选择 first - 按照配置中的顺序,选择第一个可用的 ip_hash - 基于IP地址的hash值分配 uri_hash - 基于URL的hash值分配 header - 基于某个请求头的hash值分配","categories":[{"name":"服务器","slug":"服务器","permalink":"http://yoursite.com/categories/服务器/"},{"name":"Caddy","slug":"服务器/Caddy","permalink":"http://yoursite.com/categories/服务器/Caddy/"}],"tags":[{"name":"服务器","slug":"服务器","permalink":"http://yoursite.com/tags/服务器/"},{"name":"Caddy","slug":"Caddy","permalink":"http://yoursite.com/tags/Caddy/"}]},{"title":"Caddy标准HTTP指令简介","slug":"Caddy标准HTTP指令简介","date":"2018-03-05T08:35:12.000Z","updated":"2018-03-05T08:48:14.314Z","comments":true,"path":"2018/03/05/Caddy标准HTTP指令简介/","link":"","permalink":"http://yoursite.com/2018/03/05/Caddy标准HTTP指令简介/","excerpt":"","text":"本文将对Caddy的29个标准HTTP指令做一个汇总，并简要的描述每个指令的作用． 所有的指令的详细介绍都可以在 https://caddyserver.com/docs/ 路径下查看,如查看proxy的详细介绍可访问 https://caddyserver.com/docs/proxy 指令 说明 默认情况的处理 basicauth HTTP基本认证 bind 用于给TCP监听套接字绑定IP地址 默认绑定在为通配符地址 browse 目录浏览功能 errors 配置HTTP错误页面以及错误日志 响应码&gt;=400的返回一个纯文本错误消息,也不记录日志 expvar 将运行时或者当前进程的一些信息(内存统计,启动命令,协程数等)以JSON格式暴露在某个路径下. ext 对于不存在的路径,自动追加后缀名后再次尝试 fastcgi fastcgi配置 gzip gzip压缩配置 不压缩,但是如果网站目录下存在.gz或者.br压缩文件,Caddy就会使用. 如果客户端支持gzip格式的压缩压缩文件,Caddy确保不压缩图片,视频和已压缩文件 header 设置响应头,可以增加,修改和删除.如果是代理的必须在proxy指令中设置 import 从其他文件或代码段导入配置,减少重复 index 索引文件配置 index(default).html(htm/txt) internal X-Accel-Redirect 静态转发配置, 该路径外部不可访问,caddy配置的代理可以发出X-Accel-Redirect请求 limit 设置HTTP请求头( one limit applies to all sites on the same listener)和请求体的大小限制 默认无限制.设置了之后,如果超出了限制返回413响应 log 请求日志配置 markdown 将markdown文件渲染成HTML mime 根据响应文件扩展名设置Content-Type字段 on 在服务器启动/关闭/刷新证书的时候执行的外部命令 pprof 在某个路径下展示profiling信息 proxy 反向代理和负载均衡配置 push 开启和配置HTTP/2服务器推 redir 根据请求返回重定向响应(可自己设置重定向状态码) request_id 生成一个UUID,之后可以通过{request_id}占位符使用 rewrite 服务器端的重定向 root 网站根目录配置 status 访问某些路径时,直接返回一个配置好的状态码 templates 模板配置 timeouts 设置超时时间:读请求的时间/读请求头的时间/写响应的时间/闲置时间(使用keep-alive时) Keep-Alive超时时间默认为5分钟 tls HTTPS配置,摘自文档的一句话: Since HTTPS is enabled automatically, this directive should only be used to deliberately override default settings. Use with care, if at all. websocket 提供一个简单的Websocket服务器 X-Accel-RedirectX-Accel-Redirect一般用于需要验证权限的文件的下载 nginx X-Accel-Redirect头的利用: 包含Python示例 占位符清单https://caddyserver.com/docs/placeholders","categories":[{"name":"服务器","slug":"服务器","permalink":"http://yoursite.com/categories/服务器/"},{"name":"Caddy","slug":"服务器/Caddy","permalink":"http://yoursite.com/categories/服务器/Caddy/"}],"tags":[{"name":"服务器","slug":"服务器","permalink":"http://yoursite.com/tags/服务器/"},{"name":"Caddy","slug":"Caddy","permalink":"http://yoursite.com/tags/Caddy/"}]},{"title":"pandas.read_sql的误导性参数chunksize","slug":"pandas-read-sql的误导性参数chunksize","date":"2017-12-07T08:34:29.000Z","updated":"2017-12-07T08:36:47.494Z","comments":true,"path":"2017/12/07/pandas-read-sql的误导性参数chunksize/","link":"","permalink":"http://yoursite.com/2017/12/07/pandas-read-sql的误导性参数chunksize/","excerpt":"","text":"官方文档的描述是该参数返回一个迭代的对象，该对象包含很多个chunksize大小的块。容易误导人的是设置chunksize之后，从数据获取数据就不会一次返回所有的数据，而是分块的返回。 更烦的是这个情况还不容易察觉。当使用sqlalchemy+pymssql连接MSSQL导出一张100W行数据的表，chunksize给人的表现看起来就是每次只获取部分，内存并没有太大变化，而当使用sqlalchemy+mysql连接MySQL时，就会发现内存疯涨，究其原因发现正是因为mysql获取了从MySQL获取所有数据到内存之后组装成结果集然后游标对该结果集进行操作。 网上有很多人遇到过同样的问题： https://stackoverflow.com/questions/18107953/how-to-create-a-large-pandas-dataframe-from-an-sql-query-without-running-out-of https://github.com/pandas-dev/pandas/issues/12265 解决办法就是使用sqlalchemy的执行命令时设置execution_options且stream_results参数为True, 或者使用服务器游标:create_engine()时设置参数server_side_cursors=True . stream_results – Available on: Connection, statement. Indicate to the dialect that results should be “streamed” and not pre-buffered, if possible. This is a limitation of many DBAPIs. The flag is currently understood only by the psycopg2, mysqldb and pymysql dialects. Server-side cursor support is available for the MySQLdb and PyMySQL dialects. From a MySQL point of view this means that the MySQLdb.cursors.SSCursor orpymysql.cursors.SSCursor class is used when building up the cursor which will receive results. The most typical way of invoking this feature is via theConnection.execution_options.stream_results connection execution option. Server side cursors can also be enabled for all SELECT statements unconditionally by passing server_side_cursors=True to create_engine(). 了解MySQL服务器游标可以看我的另一篇文章：MySQL服务器端游标详解．","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/数据库/"},{"name":"数据迁移","slug":"数据库/数据迁移","permalink":"http://yoursite.com/categories/数据库/数据迁移/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/数据库/"},{"name":"数据迁移","slug":"数据迁移","permalink":"http://yoursite.com/tags/数据迁移/"},{"name":"pandas","slug":"pandas","permalink":"http://yoursite.com/tags/pandas/"}]},{"title":"MySQL服务器端游标详解","slug":"MySQL服务器端游标详解","date":"2017-12-07T08:31:20.000Z","updated":"2017-12-07T08:34:02.529Z","comments":true,"path":"2017/12/07/MySQL服务器端游标详解/","link":"","permalink":"http://yoursite.com/2017/12/07/MySQL服务器端游标详解/","excerpt":"","text":"MySQL的服务器游标是通过C API中的mysql_stmt_attr_set() 实现的．同时服务器中存储过程的游标也采用相同的实现． MySQL服务器游标实现方式是通过临时表实现，首先将其放在Memory表中，如果数据大小超过max_heap_table_size 和 tmp_table_size ，就会变成MyISAM表，此时获取数据可能会变慢． 和客户端游标的区别客户端游标是client先发送select请求给Server后，Server根据条件筛选符合条件的记录，然后就会把记录发送到自己的发送buffer,等buffer满了就flush缓存（这里要注意的是如果client的接受缓存满了，那么Server的发送就会阻塞主，直到client的接受缓存空闲。），通过网络发送到client的接受缓存，然后Client就会从接受缓存里面逐个读取记录到resultset,直到所有记录都放到了resultset。 服务器边的游标则是mysqlclient一次从自己的接受缓存读取fetchSize个记录（如果buffer不够fetchSize也没关系，因为Server一直在向这个buffer 刷新数据）。mysqlclient获取fetchSize个记录放到mysqlclient的游标内部的数组里面，游标获取的时候是从数组里面获取数据，如果数组为空了，在向buffer获取fetchSize个记录。 特点和限制 游标是只读的，不支持UPDATE WHERE CURRENT OF 和 DELETE WHERE CURRENT OF 提交后游标就会关闭 游标是敏感的，非滚动的 游标没有名字，语句处理器表现为该游标的ＩＤ 参考资料 https://dev.mysql.com/doc/refman/5.7/en/cursor-restrictions.html https://stackoverflow.com/questions/24676111/streaming-mysql-resultset-with-fixed-number-of-results-at-a-time https://geert.vanderkelen.org/2010/simulating-server-side-cursors-with-mysql-connectorpython/ http://www.jianshu.com/p/c4b194b269c1","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/数据库/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"http://yoursite.com/categories/数据库/MySQL/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/数据库/"},{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}]},{"title":"半自动化迁移MSSQL数据到MySQL","slug":"半自动化迁移MSSQL数据到MySQL","date":"2017-12-05T13:00:43.000Z","updated":"2017-12-08T02:12:44.961Z","comments":true,"path":"2017/12/05/半自动化迁移MSSQL数据到MySQL/","link":"","permalink":"http://yoursite.com/2017/12/05/半自动化迁移MSSQL数据到MySQL/","excerpt":"","text":"本次数据迁移的流程： 手动流程：导出SQLServer的DDL，然后翻译成MySQL的DDL并且在MySQL中执行 自动流程：使用Pyetl完成数据的迁移 翻译DDL到处SQLServer中的DDL可以通过编写命令获得，也可以使用SQLServer的管理工具获得，其中管理工具最方便且靠谱，下面是使用该方式的教程： https://jingyan.baidu.com/article/3ea51489e672be52e61bba80.html 如果你要使用命令导出，可以参考：https://stackoverflow.com/a/20350000/6364963。 接下来就是手动翻译导出的脚本了，下面是MSSQL类型与MySQL类型的一些转换关系（摘自MySQL 5.7官方文档）： MSSQL Type MySQL Type Comment INT INT TINYINT TINYINT UNSIGNED flag set in MySQL，MySQL： -128到127(SIGNED)，0到255(UNSIGNED)，MSSQL默认就是无符号的。 SMALLINT SMALLINT BIGINT BIGINT BIT TINYINT(1) FLOAT FLOAT Precision value is used for storage size in both REAL FLOAT NUMERIC DECIMAL DECIMAL DECIMAL MONEY DECIMAL SMALLMONEY DECIMAL CHAR CHAR/LONGTEXT Depending on its length. MySQL Server 5.5 and above can have CHAR columns with a length up to 255 characters. Anything larger is migrated as LONGTEXT NCHAR CHAR/LONGTEXT Depending on its length. MySQL Server 5.5 and above can have VARCHAR columns with a length up to 65535 characters. Anything larger is migrated to one of the TEXT blob types. In MySQL, character set of strings depend on the column character set instead of the datatype. VARCHAR VARCHAR/MEDIUMTEXT/LONGTEXT Depending on its length. MySQL Server 5.5 and above can have VARCHAR columns with a length up to 65535 characters. Anything larger is migrated to one of the TEXT blob types. NVARCHAR VARCHAR/MEDIUMTEXT/LONGTEXT Depending on its length. MySQL Server 5.5 and above can have VARCHAR columns with a length up to 65535 characters. Anything larger is migrated to one of the TEXT blob types. In MySQL, character set of strings depend on the column character set instead of the datatype. 如果是NVARCHAR(max) 其长度为2G，需要MySQL里面的LONGTEXT DATE DATE DATETIME DATETIME DATETIME2 DATETIME Date range in MySQL is ‘1000-01-01 00:00:00.000000’ to ‘9999-12-31 23:59:59.999999’. Note: fractional second values are only stored as of MySQL Server 5.6.4 SMALLDATETIME DATETIME DATETIMEOFFSET DATETIME TIME TIME TIMESTAMP TIMESTAMP ROWVERSION TIMESTAMP BINARY BINARY/MEDIUMBLOB/LONGBLOB Depending on its length VARBINARY VARBINARY/MEDIUMBLOB/LONGBLOB Depending on its length TEXT VARCHAR/MEDIUMTEXT/LONGTEXT Depending on its length NTEXT VARCHAR/MEDIUMTEXT/LONGTEXT Depending on its length IMAGE TINYBLOB/MEDIUMBLOB/LONGBLOB Depending on its length SQL_VARIANT not migrated There is not specific support for this datatype. TABLE not migrated There is not specific support for this datatype. HIERARCHYID not migrated There is not specific support for this datatype. UNIQUEIDENTIFIER VARCHAR(64) A unique flag set in MySQL. There is not specific support for inserting unique identifier values. SYSNAME VARCHAR(160) XML TEXT 一些转换过程中的问题： identity 转化为 auto increment，如果identity设置了起始值，则需要在MySQL表上设置auto_increment = start_num,如果indentity设置了步长，则只能在MySQL存储引擎上设置，MySQL不支持单表的步长 翻译的时候结合正则表达式会快很多，我本次翻译120多张表总共花了差不多一个小时。翻译完成之后在MySQL中执行一下，我们就建立好了Schema了。接下来开始迁移数据。 如果觉得全部手动翻译太麻烦，可以借助以下工具完成一个初步的翻译然后自己再检验修改： http://www.sqlines.com/sql-server-to-mysql#sqlines-sql-converter-tool http://burrist.com/?page_id=535 http://sql-hub.com/Page/index.php?Shortname=amstomy 调整字符编码集需要使MySQL的字符集和SQLServer一致，查看MSSQL字符集SELECT SERVERPROPERTY(N&#39;Collation&#39;)，结果为Chinese_PRC_CI_AS，这里要注意一点，SQLServer的Collation（排序规则）不仅仅是排序规则，同时也设定了字符集。这一点和MySQL不同（MySQL字符集和排序规则是分开的）。下面是MySQL的字符集相关命令： SHOW CHARACTER SET; --查看所有支持的字符集及其默认排序规则SHOW COLLATION; --查看所有排序规则SHOW VARIABLES LIKE 'character%'; --查看系统设置的字符集及其默认排序规则SHOW VARIABLES LIKE 'collation%'; --查看系统设置的排序规则 MSSQL中的Chinese_PRC_CI_AS表示UNICODE字符集，我们需要在MySQL中使用utf-8即可，至于排序规则就先使用utf-8默认的排序规则。修改MySQL字符集可以使用set命令也可以修改my.ini配置文件并且重启服务器，具体方式可以参考：http://database.51cto.com/art/201010/229167.htm ，修改后不会影响已经存在的表和列，因此可能需要修改已存在的库，表和列的字符编码：http://www.jianshu.com/p/a18269a4870e 。注意set只在当前连接中有效，且列的默认编码继承表，表继承库。 迁移数据建立好完整的Schema之后就可以开始迁移数据了。数据迁移的方式有很多种，下面是我用过的一些方式： 编写程序，同时连接两边的数据库，从一边读取，一边插入。该方式主要缺点是迁移速度慢，数据量较大时可能几个小时才能完成一次迁移（笔者一张800W数据的表迁移了4个多小时，当然当天的网络也比较慢），迁移速度低的主要原因是网络以及INSERT语句的开销。 从一个数据库导出文件，然后拷贝到另一个数据库中去加载。该方法不受网速影响，同时导出和导入速度都很快。主要缺点是数据库都要支持导入导出功能且格式相同。比如：MSSQL导出文本文件的功能有限，不支持导出CSV，导出SQL文件也需要自己编写存储过程实现。而MySQL则同时支持.SQL文件和CSV的导入导出 笔者本次迁移的数据库大小接近6G，因此采用文件的方式导出导入，由于MSSQL本身功能有限，且手动导出流程繁琐，故使用pyetl程序来完成文件的导出和导入这一步。值得一提的是，pyetl支持多种不同关系型数据库文件以及CSV文件中数据的抽取，转换和迁移; 同时也支持多种模式的迁移，速度也非常快。本工具由笔者编写，有任何使用上的问题均可提出。 参考资料 https://stackoverflow.com/questions/11131958/what-is-the-maximum-characters-for-the-nvarcharmax https://stackoverflow.com/questions/168736/how-do-you-set-a-default-value-for-a-mysql-datetime-column https://stackoverflow.com/questions/46134550/mysql-set-default-id-uuid https://dev.mysql.com/doc/refman/5.7/en/alter-table.html#alter-table-character-set https://baike.baidu.com/item/Chinese_PRC_CI_AS http://www.sgact.com/Article/Details/D38D562D356E1DC43D656C1486393D45 https://segmentfault.com/q/1010000000132450 http://database.51cto.com/art/201010/229167.htm 网上有关MSSQL导出文本文件的资料，可以发现所有方式都不支持分隔符转义： 导出方式参考：https://stackoverflow.com/a/14266993/6364963 bcp文档：https://docs.microsoft.com/en-us/sql/tools/bcp-utility bcp要使用-U -P制定用户密码，如果不设置-c，则会要求输入一系列选项，使用-w保存Unicode,-t设置分隔符，参考：http://www.cnblogs.com/jeffry/p/5620385.html https://stackoverflow.com/questions/425379/how-to-export-data-as-csv-format-from-sql-server-using-sqlcmd/2426853#2426853 https://stackoverflow.com/questions/799465/how-to-export-sql-server-2005-query-to-csv MSSQL 不支持CSV的官方声明： https://docs.microsoft.com/en-us/sql/relational-databases/import-export/bulk-import-and-export-of-data-sql-server MySQL为什么LOAD FILE比INSERT更快： https://stackoverflow.com/questions/3635166/how-to-import-csv-file-to-mysql-table https://dba.stackexchange.com/questions/16809/why-is-load-data-infile-faster-than-normal-insert-statements","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/数据库/"},{"name":"数据迁移","slug":"数据库/数据迁移","permalink":"http://yoursite.com/categories/数据库/数据迁移/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/数据库/"},{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"},{"name":"数据迁移","slug":"数据迁移","permalink":"http://yoursite.com/tags/数据迁移/"},{"name":"MSSQL","slug":"MSSQL","permalink":"http://yoursite.com/tags/MSSQL/"},{"name":"Pyetl","slug":"Pyetl","permalink":"http://yoursite.com/tags/Pyetl/"}]},{"title":"使用NuGet 安装 MySQL Connector/NET","slug":"使用NuGet-安装-MySQL-Connector-NET","date":"2017-12-05T12:28:35.000Z","updated":"2017-12-05T13:02:23.881Z","comments":true,"path":"2017/12/05/使用NuGet-安装-MySQL-Connector-NET/","link":"","permalink":"http://yoursite.com/2017/12/05/使用NuGet-安装-MySQL-Connector-NET/","excerpt":"","text":"环境要求本次安装的环境要求： MySQL Connector/Net 6.8.x MySQL Server 5.1 or above Entity Framework 6 assemblies .NET Framework 4.0 or above 安装过程使用命令： Install-Package EntityFrameworkInstall-Package MySql.Data.Entity 如果不使用命令安装，在NuGet管理界面中搜索MySQL包时会看到好几个相关的包（都是Oracle出品），有： MySql.Data MySql.Data.Entity MySql.Web … 这里同样选择MySql.Data.Entity即可，安装该包会自动安装MySql.Data. 本次安装时MySql.Data.Entity的最新版本为6.10.4,安装完成后出现了下面这个错误，降级版本到6.8.8后即可。 Inheritance security rules violated by type: 'MySql.Data.MySqlClient.MySqlProviderServices'. Derived types must either match the security accessibility of the base type or be less accessible. 参考资料 http://lvasquez.github.io/2014/11/18/EntityFramework-MySql/ .NET 程序集查找路径 MySQL version 7.0.6-IR3 issue when try to connect to db","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://yoursite.com/categories/编程语言/"},{"name":"C#","slug":"编程语言/C","permalink":"http://yoursite.com/categories/编程语言/C/"}],"tags":[{"name":"编程语言","slug":"编程语言","permalink":"http://yoursite.com/tags/编程语言/"},{"name":"C#","slug":"C","permalink":"http://yoursite.com/tags/C/"}]},{"title":"迁移MSSQL到MySQL-使用Workbench","slug":"迁移MSSQL到MySQL-使用Workbench","date":"2017-11-28T09:26:19.000Z","updated":"2017-12-05T12:35:48.974Z","comments":true,"path":"2017/11/28/迁移MSSQL到MySQL-使用Workbench/","link":"","permalink":"http://yoursite.com/2017/11/28/迁移MSSQL到MySQL-使用Workbench/","excerpt":"","text":"官方文档教程：使用Workbench迁移MSSQL数据到MySQL，该流程需要用到iodbc，安装一下： sudo apt-get install iodbcsudo apt-get install libiodbc2-dev 之后按着教程走，在Fetch Schemas List这一步遇到问题，Google一下发现2年前有人遇到同样的问题，解决办法是升级Workbench到最新版本，然后我已经是最新版本了。 怎么Google都找不到解决办法，最后看了一篇文章：How-To: Guide to Database Migration from Microsoft SQL Server using MySQL Workbench ，此文推荐使用Native Driver驱动而不是FreeTDS驱动，于是我去下载Linux ODBC Driver，结果安装的第一步就失败，出现各种问题。 之后放弃了这种方式，决定找一台Windows装Workbench来避免安装Driver，先是使用了一台Windows服务器，安装完之后，执行迁移的时候告诉我不支持该平台，又换了一台Windows PC，成功安装，迁移时终于没有遇到之前的BUG了。 但是！！！进入下一步的时候又遇到了新的BUG，这个BUG一年前已经有人提到了，也没有什么解决方案。看了看Workbench的迁移流程总共有14步，结果在第3步和第4步连续遇到两个BUG，遂放弃Workbench迁移的方案了。 看了看其他工具，发现要么收费，要么太老，而且也不知道会不会有什么烦人的BUG。最后放弃了使用这些工具的想法了，自己写个工具来迁移吧。 http://www.dbload.com/ 只支持Windows https://www.webyog.com/product/sqlyogpricing 只支持Windows https://www.navicat.com/en/products/navicat-premium 支持Windows和Mac","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/数据库/"},{"name":"数据迁移","slug":"数据库/数据迁移","permalink":"http://yoursite.com/categories/数据库/数据迁移/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/数据库/"},{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"},{"name":"数据迁移","slug":"数据迁移","permalink":"http://yoursite.com/tags/数据迁移/"},{"name":"MSSQL","slug":"MSSQL","permalink":"http://yoursite.com/tags/MSSQL/"},{"name":"MySQL Workbench","slug":"MySQL-Workbench","permalink":"http://yoursite.com/tags/MySQL-Workbench/"}]},{"title":"IIS7体系结构简介","slug":"IIS7体系结构简介","date":"2017-11-17T08:56:38.000Z","updated":"2017-11-17T10:08:12.614Z","comments":true,"path":"2017/11/17/IIS7体系结构简介/","link":"","permalink":"http://yoursite.com/2017/11/17/IIS7体系结构简介/","excerpt":"","text":"IIS中的HTTP请求处理本节中的图表提供了正在处理的HTTP请求的概述。 以下列表描述了图中显示的请求处理流程： 当客户端浏览器向Web服务器上的资源发起一个HTTP请求时，HTTP.sys拦截该请求。 HTTP.sys联系WAS从获取配置信息。 WAS从配置存储applicationHost.config中请求配置信息。 WWW服务接收配置信息，例如应用程序池和站点配置。 WWW服务使用配置信息来配置HTTP.sys。 WAS启动进行请求的应用程序池的工作进程。 工作进程处理该请求并向HTTP.sys返回响应。 客户端收到回应。 在工作进程中，HTTP请求会通过Web Server Core中的几个有序步骤（称为事件）。在每个事件中，本地模块处理部分请求，例如验证用户或向事件日志添加信息。如果请求需要托管模块，则本地ManagedEngine模块将创建一个AppDomain，其中托管模块可以执行必要的处理，例如使用Forms身份验证对用户进行身份验证。当请求通过Web Server Core中的所有事件时，响应将返回到HTTP.sys。图2显示了一个进入工作进程的HTTP请求。 IIS组件介绍本文将介绍IIS的组件，模块和请求处理体系结构： IIS包含几个组件，这些组件在WindowsServer®2008（IIS 7.0）和Windows Server 2008 R2（IIS 7.5）中为应用程序和Web服务器角色执行重要功能。每个组件都有责任，例如监听对服务器的请求，管理进程和读取配置文件。这些组件包括协议侦听器（如HTTP.sys）和服务（如万维网发布服务（WWW服务）和Windows进程激活服务（WAS））。 协议监听器协议侦听器接收协议特定的请求，将它们发送到IIS进行处理，然后将响应返回给请求者。例如，当客户端浏览器从Internet请求Web页面时，HTTP侦听程序HTTP.sys将拾取请求并将其发送到IIS进行处理。一旦IIS处理请求，HTTP.sys就会向客户端浏览器返回响应。 默认情况下，IIS提供HTTP.sys作为侦听HTTP和HTTPS请求的协议侦听器。HTTP.sys是在IIS 6.0中作为HTTP请求的HTTP特定协议侦听器引入的。HTTP.sys仍然是IIS 7及更高版本中的HTTP侦听器，但包括对安全套接字层（SSL）的支持。 要支持使用HTTP和HTTPS以外的协议的服务和应用程序，可以使用Windows Communication Foundation（WCF）等技术。WCF具有侦听器适配器，可提供协议侦听器和侦听器适配器的功能。侦听器适配器将在本文后面介绍。有关WCF的更多信息，请参阅MSDN上的Windows Communication Foundation。 超文本传输协议栈（HTTP.sys）HTTP侦听器是Windows操作系统的网络子系统的一部分，它被实现为称为HTTP协议栈（HTTP.sys）的内核模式设备驱动程序。HTTP.sys侦听来自网络的HTTP请求，将请求传递到IIS进行处理，然后将处理后的响应返回给客户端浏览器。 在IIS 6.0中，HTTP.sys替换了Windows Sockets API（Winsock），它是由以前版本的IIS用来接收HTTP请求和发送HTTP响应的用户模式组件。IIS 7和更高版本继续依赖HTTP.sys来处理HTTP请求。 HTTP.sys提供了以下好处： 内核模式缓存。无需切换到用户模式即可提供缓存响应请求。 内核模式请求排队。请求在上下文切换中引起较少的开销，因为内核将请求直接转发给正确的工作进程。如果没有工作进程可用于接受请求，则内核模式请求队列将保留该请求，直到工作进程将其请求为止。 请求预处理和安全过滤。 万维网发布服务（WWW服务）在IIS 7和更高版本中，以前由万维网发布服务（WWW服务）单独处理的功能现在分为两个服务：WWW服务和一个新服务Windows进程激活服务（WAS）。这两个服务在同一Svchost.exe进程中作为LocalSystem运行，并共享相同的二进制文件。 注意您也可以在文档中看到称为W3SVC的WWW服务。 WWW服务如何在IIS中工作在IIS中，WWW服务不再管理工作进程。相反，WWW服务是HTTP侦听器HTTP.sys的侦听器适配器。作为监听适配器，WWW服务主要负责配置HTTP.sys，在配置更改时更新HTTP.sys，并在请求进入请求队列时通知WAS。 此外，WWW服务继续收集网站的计数器。因为性能计数器仍然是WWW服务的一部分，所以它们是HTTP特定的，不适用于WAS。 Windows进程激活服务（WAS）在IIS 7和更高版本中，Windows进程激活服务（WAS）管理应用程序池配置和工作进程，而不是WWW服务。这使您可以为HTTP和非HTTP站点使用相同的配置和流程模型。 另外，如果您不需要HTTP功能，则可以在不使用WWW服务的情况下运行WAS。例如，如果您不需要在HTTP.sys中侦听HTTP请求，则可以通过WCF侦听器适配器（如NetTcpActivator）管理Web服务，而无需运行WWW服务。有关WCF侦听器适配器以及如何使用WAS在IIS 7和更高版本中托管WCF应用程序的信息，请参阅MSDN上的WCF托管。 WAS中的配置管理在启动时，WAS从ApplicationHost.config文件读取某些信息，并将该信息传递给服务器上的侦听器适配器。侦听器适配器是在WAS和协议侦听器（如HTTP.sys）之间建立通信的组件。一旦侦听器适配器接收到配置信息，它们将配置其相关协议侦听器，并准备侦听器侦听请求。 在WCF的情况下，侦听器适配器包含协议侦听器的功能。因此，WCF侦听器适配器（如NetTcpActivator）是基于来自WAS的信息进行配置的。一旦配置了NetTcpActivator，它将侦听使用net.tcp协议的请求。有关WCF侦听器适配器的更多信息，请参阅MSDN上的WAS激活体系结构。 以下列表描述了WAS从配置中读取的信息的类型： 全局配置信息 HTTP和非HTTP协议的协议配置信息 应用程序池配置，例如进程帐户信息 站点配置，如绑定和应用程序 应用程序配置，例如应用程序所属的已启用的协议和应用程序池 如果ApplicationHost.config发生更改，WAS将收到通知并使用新信息更新侦听器适配器。 流程管理WAS管理HTTP和非HTTP请求的应用程序池和工作进程。当协议侦听器拿起客户端请求时，WAS将确定工作进程是否正在运行。如果应用程序池已经有一个正在处理请求的工作进程，那么侦听器适配器会将请求传递到工作进程上进行处理。如果应用程序池中没有工作进程，则WAS将启动一个辅助进程，以便侦听适配器可以将请求传递给它进行处理。 注意：因为WAS管理HTTP和非HTTP协议的进程，所以可以在同一个应用程序池中运行具有不同协议的应用程序。例如，您可以开发一个应用程序（如XML服务），并通过HTTP和net.tcp进行托管。 在IIS中的模块IIS提供了一个与以前版本的IIS不同的新体系结构。IIS不包含服务器本身的大部分功能，而是包含一个Web服务器引擎，您可以在其中添加或删除称为模块的组件，具体取决于您的需要。 模块是服务器用于处理请求的个别功能。例如，IIS使用身份验证模块来验证客户端凭据，并使用缓存模块来管理缓存活动。 新的体系结构比以前版本的IIS提供了以下优点： 你可以在服务器上控制你想要的模块。 您可以将服务器自定义为您环境中的特定角色。 您可以使用自定义模块来替换现有模块或引入新功能。 新的架构也提高了安全性并简化了管理。通过删除不必要的模块，可以减少服务器的攻击面和内存占用量，这是服务器工作进程在计算机上使用的内存量。您还可以不必管理您的站点和应用程序不需要的功能。 IIS中的请求处理在IIS中，IIS和ASP.NET请求管道结合使用集成方法处理请求。新的请求处理体系结构包括一个本地和托管模块的有序列表，它们根据请求执行特定的任务。 与以前版本的IIS相比，此设计提供了几个优点。首先，所有文件类型都可以使用原来只能用于托管代码的功能。例如，您现在可以对站点和应用程序中的静态文件，活动服务器页面（ASP）文件以及所有其他文件类型使用ASP.NET窗体身份验证和统一资源定位器（URL）授权。 其次，这种设计消除了IIS和ASP.NET中几个功能的重复。例如，当客户端请求托管文件时，服务器将调用集成管道中的相应认证模块来认证客户端。在以前的IIS版本中，这个相同的请求将通过IIS管道和ASP.NET管道中的身份验证过程。 第三，你可以在一个位置管理所有的模块，而不是在IIS中管理一些功能，在ASP.NET配置中管理一些功能。这简化了服务器上的站点和应用程序的管理。 在IIS中的应用程序池应用程序按进程边界分隔应用程序，以防止应用程序影响服务器上的另一个应用程序。在IIS 7和更高版本中，应用程序池继续使用IIS 6.0工作进程隔离模式。另外，您现在可以指定一个设置来确定如何处理涉及受管资源的请求：集成模式或经典模式。 注意：在IIS 6.0中，在服务器级别设置工作进程隔离模式和IIS 5.0隔离模式。这使得无法在同一台服务器上运行两种隔离模式。但是，在IIS 7和更高版本中，应用程序池级别设置了集成模式和经典模式，这使您可以在同一服务器上的不同进程模式的应用程序池中同时运行应用程序。 集成应用程序池模式当应用程序池处于集成模式时，可以利用IIS和ASP.NET的集成请求处理体系结构。当应用程序池中的工作进程收到请求时，请求会通过一个有序的事件列表。每个事件都会调用必要的本地和托管模块来处理部分请求并生成响应。 在集成模式下运行应用程序池有几个好处。首先将IIS和ASP.NET的请求处理模型集成到统一的流程模型中。此模型消除了以前在IIS和ASP.NET中重复的步骤，如身份验证。此外，集成模式可以使所有内容类型的托管功能可用。 经典应用程序池模式当应用程序池处于经典模式时，IIS 7和更高版本以与IIS 6.0工作进程隔离模式中相同的方式处理请求。ASP.NET请求首先会经过IIS中的本机处理步骤，然后被路由到Aspnet_isapi.dll以处理托管运行时中的托管代码。最后，请求通过IIS路由回来发送响应。 IIS和ASP.NET请求处理模型的这种分离会导致一些处理步骤（如身份验证和授权）的重复。此外，托管代码功能（例如，窗体身份验证）仅适用于ASP.NET应用程序或您已将脚本映射到由aspnet_isapi.dll处理的所有请求的应用程序。 在将生产环境升级到IIS 7及更高版本并将应用程序分配到集成模式下的应用程序池之前，请确保在集成模式下测试现有应用程序的兼容性。如果应用程序无法在集成模式下工作，则应该只在Classic模式下将应用程序添加到应用程序池。例如，您的应用程序可能依赖于从IIS传递到托管运行时的身份验证令牌，并且由于IIS 7及更高版本中的新架构，进程会中断您的应用程序。 参考资料 https://www.iis.net/learn/get-started/introduction-to-iis/introduction-to-iis-architecture http://www.cnblogs.com/andyyu/archive/2010/07/22/1782998.html http://www.uml.org.cn/net/201603172.asp","categories":[{"name":"服务器","slug":"服务器","permalink":"http://yoursite.com/categories/服务器/"},{"name":"IIS","slug":"服务器/IIS","permalink":"http://yoursite.com/categories/服务器/IIS/"}],"tags":[{"name":"服务器","slug":"服务器","permalink":"http://yoursite.com/tags/服务器/"},{"name":"IIS","slug":"IIS","permalink":"http://yoursite.com/tags/IIS/"}]},{"title":"OpenCV 表格文字识别","slug":"OpenCV-表格文字识别","date":"2017-11-15T10:38:36.000Z","updated":"2017-11-15T10:46:34.572Z","comments":true,"path":"2017/11/15/OpenCV-表格文字识别/","link":"","permalink":"http://yoursite.com/2017/11/15/OpenCV-表格文字识别/","excerpt":"","text":"最近项目需要识别图片中的表格文字，遂学习了一波OpenCV。现在将整个流程记录下来以供交流和分享。 本次的目标是识别下图中的表格，最终需要知道每一部分填写的内容内容是什么，比如知道用户电话是13988881234。我将此次任务划分为3个步骤： 识别出每一个单元格，得到其矩形区域的位置 知道每一个矩形区域的意义。比如知道e矩形区域代表用户的电话号码 对该区域的图片进行文字识别 本文将要讲解的就是如何完成第一步：得到所有矩形区域的位置。其余步骤可能会在之后的文章进行讲解。我采用的语言是Python，即使你不懂Python也应该能看懂流程，因为Python非常易读。我采用的OpenCV为当前的最新版3.3.1。 如果你是Linux用户且需要安装OpenCV及其相关的Python库，请看我的另一篇文章：Linux OpenCV安装指南 原理我们知道OpenCV提供了轮廓识别的API：findContours，根据该API的描述，在轮廓检测之前，首先要对图片进行二值化或者Canny边缘检测。寻找的物体是白色的，而背景必须是黑色的。 经过测试后使用二值化的方式对图片进行处理比较好，要注意的是调用threshold时的阈值方法要使用THRESH_BINARY_INV（因为我们需要变成白色，背景变成黑色）。 其次经过二值化处理之后我发现有些边框会出现一些小的空隙，可能是打印问题或者其他因素造成的。对于这种细小的空隙我采用对二值化后的图片进行一次膨胀操作进行去除。 最后一步就是检测轮廓，使用findContours方法，并且轮廓检索模式（Contour retrieval mode）使用RETR_TREE，使用模式我们不仅可以得到所有的轮廓，同时还可以得到轮廓之间的关系（父子关系，兄弟关系）检测出所有轮廓之后，我们需要找出其中最大的轮廓，它代表表格的外边框，然后其所有的子轮廓就是我们要找的内容。 这一步遇到的问题是OpenCV会识别出多余的轮廓，或者识别出的轮廓形状不像长方形。为了防止识别出的多余的轮廓，我们采用面积过滤，估算一下表格中最小的单元格区域的面积，然后所有小于该面积的都过滤掉。对于轮廓不像长方形，我们采用boundingRect得到包含该区域的最小矩形。 代码# -*- coding: utf-8 -*-import numpy as npimport cv2img = cv2.imread('1.jpg')# 灰度图gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)# 阈值_, threshold = cv2.threshold(gray, 100, 255, cv2.THRESH_BINARY_INV)# 膨胀kernel = np.ones((3, 3), np.uint8)dilated = cv2.dilate(threshold, kernel, iterations=1)# 轮廓检测_, contours, hierarchy = cv2.findContours(dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)# 找到表格最外层轮廓tree = hierarchy[0]max_size = 0outer_contour_index = Nonefor i, contour in enumerate(contours): size = cv2.contourArea(contour) if size &gt; max_size: max_size = size outer_contour_index = icontour = contours[outer_contour_index]# 估算最小的单元格大小x, y, w, h = cv2.boundingRect(contour)cell_size = w * h * 0.0015# 遍历所有最外层轮廓的子轮廓，过滤掉面积小于cell_size的，然后全部用矩形标注起来contour_index = tree[outer_contour_index][2]count = 0while 1: contour = contours[contour_index] if cv2.contourArea(contour) &gt; cell_size: x, y, w, h = cv2.boundingRect(contour) count += 1 cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), 3) contour_index = tree[contour_index][0] if contour_index &lt; 0: cv2.imwrite('11.jpg', img) break 效果图 参考链接 Image Processing (imgproc module)","categories":[{"name":"CS","slug":"CS","permalink":"http://yoursite.com/categories/CS/"},{"name":"图像处理","slug":"CS/图像处理","permalink":"http://yoursite.com/categories/CS/图像处理/"}],"tags":[{"name":"CS","slug":"CS","permalink":"http://yoursite.com/tags/CS/"},{"name":"图像处理","slug":"图像处理","permalink":"http://yoursite.com/tags/图像处理/"},{"name":"OpenCV","slug":"OpenCV","permalink":"http://yoursite.com/tags/OpenCV/"}]},{"title":"Linux OpenCV安装指南","slug":"Linux-OpenCV安装指南","date":"2017-11-09T06:12:15.000Z","updated":"2017-11-15T10:53:53.061Z","comments":true,"path":"2017/11/09/Linux-OpenCV安装指南/","link":"","permalink":"http://yoursite.com/2017/11/09/Linux-OpenCV安装指南/","excerpt":"","text":"本教程是笔者在Linux Mint 16.1（基于Ubuntu 16.04）下测试后给出的指南，理论情况下也适用于其他发行版。 安装依赖[compiler] sudo apt-get install build-essential[required] sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev[optional] sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev libgtk2.0-dev依赖问题安装过程中可能出现如下问题： 下列软件包有未满足的依赖关系： libgtk2.0-dev : 依赖: libgdk-pixbuf2.0-dev (&gt;= 2.21.0) 但是它将不会被安装 依赖: libpango1.0-dev (&gt;= 1.20) 但是它将不会被安装 依赖: libcairo2-dev (&gt;= 1.6.4-6.1) 但是它将不会被安装 如果出现此问题，则以此执行下列命令即可： sudo apt install libpng12-0=1.2.54-1ubuntu1sudo apt-get install libpng12-dev sudo apt-get install libgdk-pixbuf2.0-dev sudo apt-get install libgtk2.0-dev 安装OpenCV下载源码到该页面下载需要的版本：https://opencv.org/releases.html 编译安装cd opencv-*mkdir buildcd buildcmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local .. 如果cmake过程中出现了下面这个问题： CMake Error at CMakeLists.txt:11 (message): FATAL: In-source builds are not allowed. You should create a separate directory for build files.-- Configuring incomplete, errors occurred! 删除掉opencv-**目录下的CMakeCache.txt*，之后在执行安装即可。 安装Python包另外要使用Python调用OpenCV需要安装的python-opencv包: sudo apt-get install python-opencv 参考链接 http://blog.csdn.net/FogXcG/article/details/75808783 https://docs.opencv.org/master/d7/d9f/tutorial_linux_install.html https://stackoverflow.com/questions/45518317/in-source-builds-are-not-allowed-in-cmake","categories":[{"name":"CS","slug":"CS","permalink":"http://yoursite.com/categories/CS/"},{"name":"图像处理","slug":"CS/图像处理","permalink":"http://yoursite.com/categories/CS/图像处理/"}],"tags":[{"name":"CS","slug":"CS","permalink":"http://yoursite.com/tags/CS/"},{"name":"图像处理","slug":"图像处理","permalink":"http://yoursite.com/tags/图像处理/"},{"name":"OpenCV","slug":"OpenCV","permalink":"http://yoursite.com/tags/OpenCV/"}]},{"title":"统计机器翻译（SMT）简介","slug":"统计机器翻译（SMT）简介","date":"2017-11-08T06:56:34.000Z","updated":"2017-11-08T08:57:10.122Z","comments":true,"path":"2017/11/08/统计机器翻译（SMT）简介/","link":"","permalink":"http://yoursite.com/2017/11/08/统计机器翻译（SMT）简介/","excerpt":"","text":"统计机器翻译（英语：Statistical Machine Translation，简写为SMT）是机器翻译的一种，基本思想是通过对大量的平行语料进行统计分析，构建模型，进而使用此模型进行翻译。 所谓的平行语料就是表达同一个意思的不同语言的句子，如： 這是一個蘋果。This is an apple.桌上有一本書。There is a book on the table． 模型的定义假设一个源语言句子$f$和一个目标语言句子$e$，我们定义$f$翻译成$e$的概率为：$$p(e|f)$$于是如何将$f$翻译成$e$的问题就变成了求解$p(e|f)$最大的时候的$e$。由贝叶斯定理可知:$$p(e \\mid f) = \\frac{p(f \\mid e)p(e)}{p(f)}$$由于$f$句子是已知的，所以$p(f)$是一个常数，因此求解$p(e|f)$最大的时候的$e$就是求解$p(f \\mid e)p(e)$最大的时候的$e$。用公式表示就是：$$\\mathop{\\arg\\,\\max}\\limits{e} p(e \\mid f) = \\mathop{\\arg\\,\\max}\\limits{e} p(e)\\times p(f \\mid e)$$ 举个例子，假设要将“我肚子饿了”翻译，并且我们已经根据模型计算出了可能翻译的句子，如下：$$\\begin{array}{c | c c}English &amp; p(f \\mid e) &amp; p (e) \\ \\hline\\text{I am hungry} &amp; 0.00019 &amp; 0.0084\\\\text{My belly hungry} &amp; 0.00031 &amp; 0.0000031\\\\text{I starve} &amp; 0.00045 &amp; 0.0000012\\\\end{array}$$ 那么这句中文最有可能的翻译则为“I am hungry”。因为它的$ p(e)\\times p(f \\mid e)$最大。在统计机器翻译中，我们把$ p(e)$叫做语言模型，表示一个句子是一个流畅的句子的概率，比如上面“I am hungry”就比另外两个流程;我们把$ p(f \\mid e)$叫做翻译模型，表示把句子从$e$翻译到$f$的概率。从中国对翻译的传统要求“信达雅”三点上看，翻译模型体现了信与达，而雅则在语言模型中得到反映。 因此我们的主要任务就是训练模型，让它知道各种情况下$ p(f \\mid e)$的值和$ p(e)$的值。 语言模型假如$e$这个句子由$w1$，$w2$…$wn$这些单词组成的，那么可知：$$p(e)=p(w1)p(w2|w1)p(w3|w1,w2)…p(wn|w1,w2,…,wn-1)$$即：$e$这个句子出现的概率等于第一个单词出现的概率$\\times$第一个单词出现的情况下第二个单词出现的概率$\\times$第一个单词和第二个单词均出现的情况下第三个单词出现的概率… 而以上的各个单词在其前面的单词出现的情况下出现的概率是可以通过语料库统计出来的。这里就解决了$p(e)$计算的问题。 随着句子单词数量增多，$p(wn|w1,w2,…,wn-1)$计算将会变得非常耗时，所以一般实际中采取n-gram求近似值。n一般不超过3, google使用的是4. 翻译模型我们假设$e$由$(e1,e2…,el)$这些单词按顺序组成，$f$由$(f1,f2,…,fj)$这些单词按照顺序组成：那么$p(f \\mid e)$这个问题可以看成$(e1,e2…,el)$到$(f1,f2,…,fj)$的概率问题。第一个句子如何转换为第二个句子呢，我们假设这之间存在一种转换A，那么:$$P(F\\mid E) = \\sum{A} P(F,A \\mid E) = \\sum{A} P(F \\mid E, A) \\times P(A \\mid E)$$现在我们假设这种转换A就是$e$中的一个单词$el$到$f$中的一个单词$fj$的对齐转换。我们规定$e$中的每个单词可以对应$f$中的0-1个单词（因为有的$f$中的词可能在$e$中不存在，比如中文的”的”在英文中就找不到对应的单词），那么有：$$P(F \\mid E, A) = \\prod{j=1}^{J} t(f{j} \\mid e{A{j}})$$其中，$t(fj∣eAj)$ 表示在 alignment A 之下，对应到$fj$位置的$ej$ ，翻譯成 $fj$ 的機率。因为两个句子之间的对齐关系有很多种，现在我们假设每种对齐的概率都一样，那么：$$P(A \\mid E) = \\frac{\\epsilon}{(I+1)^{J}}$$将这两项代人原来的公式得到（这就是IBM Model 1）：$$\\begin{equation} P(F\\mid E) = \\sum{A} P(F,A \\mid E) = \\sum{A} P(F \\mid E, A) \\times P(A \\mid E) \\ = \\sum{A} \\prod{j=1}^{J} t(f{j} \\mid e{A_{j}}) \\times \\frac{\\epsilon}{(I+1)^{J}} \\\\end{equation}$$我们只需要先求出该公式里面的各个参数，就可以使用该模型来进行翻译了。求解其中参数的问题这里就不描述了，感兴趣的可以去看一些$EF$算法。另外，$IBM Model 2$在$1$的基础上去掉所有对齐概率相等的假设并加入了新的参数：词在句子中的位置，$HMM$模型将$IBM Model 2$中的绝对位置更改为相对位置，即相对上一个词连接的位置，而IBM Model 3,4,5及Model 6引入了“Fertility Model”，代表一个词翻译为若干词的概率。 参考链接 机器翻译 – Statistical Machine Translation - MARK CHANG’S BLOG 机器翻译 – IBM Model 1 - MARK CHANG’S BLOG n元语法 - 维基百科 统计机器翻译 - 维基百科 基于词的统计机器翻译方法 - 中国科学院计算技术研究所 2010 年秋季课程","categories":[{"name":"CS","slug":"CS","permalink":"http://yoursite.com/categories/CS/"},{"name":"自然语言处理","slug":"CS/自然语言处理","permalink":"http://yoursite.com/categories/CS/自然语言处理/"}],"tags":[{"name":"CS","slug":"CS","permalink":"http://yoursite.com/tags/CS/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://yoursite.com/tags/自然语言处理/"},{"name":"机器翻译","slug":"机器翻译","permalink":"http://yoursite.com/tags/机器翻译/"}]},{"title":"【Redis实战】存储24小时内的操作历史","slug":"【Redis实战】存储24小时内的操作历史","date":"2017-11-03T03:58:23.000Z","updated":"2017-11-08T08:14:51.212Z","comments":true,"path":"2017/11/03/【Redis实战】存储24小时内的操作历史/","link":"","permalink":"http://yoursite.com/2017/11/03/【Redis实战】存储24小时内的操作历史/","excerpt":"","text":"分析与实现一个消息应用有如下需求： 需要保留用户最近24小时内的所有操作历史 需要频繁的显示最近的10条操作历史给用户 需要频繁的显示24小时内总操作次数 这是一个典型的适合应用Redis的情形。现在需要考虑的是如何具体实现，首先想到的是采用字典（哈希表）列表的方式实现，其中每一个哈希表存储操作内容和操作时间，同时包含一个过期时间。但是考虑到Redis支持设置键的过期时间，所以哈希表本身可以不用存储过期时间，而是在创建的时候设置好过期时间即可。 我们给操作历史列表取名为history,之后每当有新的操作发生的时候我们需要在列表中新增一项，同时创建一个新的哈希表： 127.0.0.1:6379&gt; RPUSH history operation:1(integer) 1127.0.0.1:6379&gt; HMSET operation:1 time 12:23 desc o1OK127.0.0.1:6379&gt; EXPIRE operation:1 30(integer) 1 这样，当需要统计总操作数或者需要显示最近的10条记录的时候，我们需要先遍历history列表，然后删除掉其中已经过期的哈希表的键，最后再返回总条数或者最近10条记录。 为了减少网络传输时间，我们将删除过期键的功能使用Lua脚本实现并加载到Redis(&gt;2.6.0)中。Lua脚本： while (true)do if (redis.call('EXISTS', redis.call('LRANGE', KEYS[1], 0, 0)[1]) == 1) then break else if redis.call('LLEN', KEYS[1]) == 0 then break end redis.call('LPOP', KEYS[1]) endendlocal ret = &#123; 'ok' &#125;ret['ok'] = 'OK'return ret 查看本例完整的Python实现可访问Gist。 时间复杂度首先列出我们使用的各个操作的时间复杂度： 操作 时间复杂度 LPUSH O(1) HMSET O(N)， N 为 field-value 对的数量。 EXPIRE O(1) EXISTS O(1) LRANGE O(S+N)， S 为偏移量 start ， N 为指定区间内元素的数量。 LLEN O(1) LPOP O(1) 具体到我们实现的功能上来说，各个功能的时间复杂度如下： 功能 复杂度 新增一条操作记录 O(N)，N为hash中key的数量 删除过期的记录 O(N)，N为本次过期条数 取最新的10条记录 O(1) 参考链接避免误用 Redis Redis 命令参考 Redis Lua 脚本使用 Lua: A Guide for Redis Users How to Create and Expire List Items in Redis","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/数据库/"},{"name":"Redis","slug":"数据库/Redis","permalink":"http://yoursite.com/categories/数据库/Redis/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/数据库/"},{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}]},{"title":"深入理解BFC和Margin Collapse","slug":"深入理解BFC和Margin-Collapse","date":"2017-11-01T14:19:05.000Z","updated":"2017-11-01T14:29:00.181Z","comments":true,"path":"2017/11/01/深入理解BFC和Margin-Collapse/","link":"","permalink":"http://yoursite.com/2017/11/01/深入理解BFC和Margin-Collapse/","excerpt":"","text":"BFC定义浮动元素和绝对定位元素，非块级盒子的块级容器（例如 inline-blocks, table-cells, 和 table-captions），以及overflow值不为“visiable”的块级盒子，都会为他们的内容创建新的BFC（块级格式上下文）。 在BFC中，盒子从顶端开始垂直地一个接一个地排列，两个盒子之间的垂直的间隙是由他们的margin 值所决定的。在一个BFC中，两个相邻的块级盒子的垂直外边距会产生塌陷。 在BFC中，每一个盒子的左外边缘（margin-left）会触碰到容器的左边缘(border-left)（对于从右到左的格式来说，则触碰到右边缘）。 Margin Collapse(外边距塌陷/外边距合并)在CSS当中，相邻的两个盒子（可能是兄弟关系也可能是祖先关系）的外边距可以结合成一个单独的外边距。这种合并外边距的方式被称为塌陷，并且因而所结合成的外边距称为塌陷外边距。 塌陷的结果 两个相邻的外边距都是正数时，塌陷结果是它们两者之间较大的值。 两个相邻的外边距都是负数时，塌陷结果是两者绝对值的较大值。 两个外边距一正一负时，塌陷结果是两者的相加的和。 产生塌陷的必备条件margin必须是邻接的，而根据w3c规范，两个margin是邻接的必须满足以下条件： 必须是处于常规文档流（非float和绝对定位）的块级盒子,并且处于同一个BFC当中。如果有元素脱离了常规文档流，那么该元素之后的元素和该元素之前的元素可以邻接。 没有线盒，没有空隙（clearance：当浮动元素之后的元素设置clear以闭合相关方向的浮动时会产生。有这个的时候，修改清除浮动元素的margin-top无效），没有padding和border将他们分隔开 都属于垂直方向上相邻的外边距，可以是下面任意一种情况 元素的margin-top与其第一个常规文档流的子元素的margin-top 元素的margin-bottom与其下一个常规文档流的兄弟元素的margin-top 元素的margin-bottom与其最后一个常规文档流的子元素的margin-bottom，同时元素的height设置为auto 高度为0并且最小高度也为0，不包含常规文档流的子元素，并且自身没有建立新的BFC的元素的margin-top和margin-bottom。 以上的条件意味着下列的规则： 创建了新的BFC的元素（例如浮动元素或者’overflow’值为’visible’以外的元素）与它的子元素的外边距不会塌陷 浮动元素不与任何元素的外边距产生塌陷（包括其父元素和子元素）。因为浮动元素产生新的BFC。 绝对定位元素不与任何元素的外边距产生塌陷 inline-block元素不与任何元素的外边距产生塌陷 一个常规文档流元素的margin-bottom与它下一个常规文档流的兄弟元素的margin-top会产生塌陷，除非它们之间存在间隙（clearance）。 一个常规文档流元素的margin-top 与其第一个常规文档流的子元素的margin-top产生塌陷，条件为父元素不包含 padding 和 border ，子元素不包含 clearance。 一个 ‘height’ 为 ‘auto’ 并且 ‘min-height’ 为 ‘0’的常规文档流元素的 margin-bottom 会与其最后一个常规文档流子元素的 margin-bottom 塌陷，条件为父元素不包含 padding 和 border ，子元素的 margin-bottom 不与包含 clearance 的 margin-top 塌陷。 一个不包含border-top、border-bottom、padding-top、padding-bottom的常规文档流元素，并且其 ‘height’ 为 0 或 ‘auto’， ‘min-height’ 为 ‘0’，其里面也不包含行盒(line box)，其自身的 margin-top 和 margin-bottom 会塌陷。 参考连接https://www.w3.org/TR/CSS22/box.html#collapsing-margins CSS: 深入理解BFC和Margin Collapse (margin叠加或者合并外边距) https://developer.mozilla.org/zh-CN/docs/Web/CSS/CSS_Box_Model/Mastering_margin_collapsing","categories":[{"name":"前端","slug":"前端","permalink":"http://yoursite.com/categories/前端/"},{"name":"CSS","slug":"前端/CSS","permalink":"http://yoursite.com/categories/前端/CSS/"}],"tags":[{"name":"前端","slug":"前端","permalink":"http://yoursite.com/tags/前端/"},{"name":"CSS","slug":"CSS","permalink":"http://yoursite.com/tags/CSS/"},{"name":"CSS盒模型","slug":"CSS盒模型","permalink":"http://yoursite.com/tags/CSS盒模型/"},{"name":"CSS布局","slug":"CSS布局","permalink":"http://yoursite.com/tags/CSS布局/"}]},{"title":"MySQL存储过程及函数核心知识点","slug":"MySQL存储过程及函数核心知识点","date":"2017-10-31T13:34:33.000Z","updated":"2018-01-16T05:46:39.774Z","comments":true,"path":"2017/10/31/MySQL存储过程及函数核心知识点/","link":"","permalink":"http://yoursite.com/2017/10/31/MySQL存储过程及函数核心知识点/","excerpt":"","text":"本文记录了MySQL存储过程的一些核心知识点，详细内容请参考官方文档。文中的MySQL版本为5.7。 语法CREATE [DEFINER = &#123; user | CURRENT_USER &#125;] PROCEDURE sp_name ([proc_parameter[,...]]) [characteristic ...] routine_bodyCREATE [DEFINER = &#123; user | CURRENT_USER &#125;] FUNCTION sp_name ([func_parameter[,...]]) RETURNS type [characteristic ...] routine_bodyproc_parameter: [ IN | OUT | INOUT ] param_name typefunc_parameter: param_name typetype: Any valid MySQL data typecharacteristic: COMMENT &apos;string&apos; | LANGUAGE SQL | [NOT] DETERMINISTIC | &#123; CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA &#125; | SQL SECURITY &#123; DEFINER | INVOKER &#125;routine_body: Valid SQL routine statement 存储过程可以加 db_name限定数据库，不加就使用默认数据库 默认情况下，MYSQL给存储过程创建者ALTER ROUTINE和EXECUTE权限。 proc_parameter默认IN，可以选择OUT，INOUT proc_parameter只支持IN characteristic COMMENT用于写备注 LANGUAGE这个server被忽略，只是为了符合SQL标准 DETERMINISTIC和NOT DETERMINISTIC。这个只是创建者自己定义的，MYSQL不会做任何检查。但是如果把NOT DETEMINISTIC定义为DETERMINISTIC，可能导致优化器做出错误的执行计划。相反，把DETEMINISTIC定义为NOT DETERMINISTIC可能让一些可用的优化措施无法使用。如果使用binary logging，需要参考Binary Logging of Stored Programs. { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA }这组被server忽略 SQL SECURITY定义谁可以调用。DEFINER表示由DEFINER属性所指定的用户的权限来执行，INVOKER表示以调用这个存储过程的用户的权限来执行。默认DEFINER，且值为CURRENT_USER。如果用户没有过程中需要的权限，那么INVOKER类型过程的就无法调用。 routine_body可以执行Compound-Statement.以及DDL语句，同时存储过程支持事务。 Compiund StatementBEGIN END语法语句块，可嵌套，可结合Lable使用。 Label语法可用于标记BEGIN，LOOP，REPEAT以及WHILE语句。可以用ITERATE和LEAVE指令控制流程。 DECLARE语法声明变量，条件处理或者游标。限制如下： 只能在BEGIN语句中，且必须在最前面 必须按照变量，游标，条件处理的顺序声明。 变量所用DECLARE声明，赋值可以使用SET语法，SELECT ... INTO var_list 或者 FETCH ... INTO var_list 定义格式： DECLARE var_name [, var_name] ... type [DEFAULT value] 控制流程和C语言的类比 MYSQL C 备注 CASE switch IF if LOOP while(1){} 用ITERATE模拟continue，LEAVE模拟break REPATE do…while 用ITERATE模拟continue，LEAVE模拟break WHILE while 用ITERATE模拟continue，LEAVE模拟break 游标游标使用的流程遵循：定义，OPEN，FETCH，CLOSE。语法如下 DECLARE cursor_name CURSOR FOR select_statementOPEN cursor_nameFETCH [[NEXT] FROM] cursor_name INTO var_name [, var_name] ...CLOSE cursor_name 条件处理程序执行过程中可能出现一些需要特殊处理的情况，比如继续执行还是推出程序？可以为一个条件定义处理器，条件也可以被命名。 命名条件使用DECLARE … CONDITION 语法，此步骤可选。 定义处理器使用DECLARE … HANDLER 语法 自己抛出一个条件使用SIGNAL 语法，定义条件处理器中继续抛出使用RESIGNAL 语法 获取错误内容使用 GET DIAGNOSTICS 语法 例子： -- 命名DECLARE division_by_zero CONDITION FOR SQLSTATE &apos;22012&apos;;-- 定义处理器以及使用RESINGAL DECLARE CONTINUE HANDLER FOR division_by_zero RESIGNAL SET MESSAGE_TEXT = &apos;Division by zero / Denominator cannot be zero&apos;; -- GET DIAGNOSTICS语法DROP TABLE test.no_such_table;-- ERROR 1051 (42S02): Unknown table &apos;test.no_such_table&apos;GET DIAGNOSTICS CONDITION 1 @p1 = RETURNED_SQLSTATE, @p2 = MESSAGE_TEXT;SELECT @p1, @p2;+-------+------------------------------------+| @p1 | @p2 |+-------+------------------------------------+| 42S02 | Unknown table &apos;test.no_such_table&apos; |+-------+------------------------------------+ 常用的处理器： 常见问题有许多常见问题是因为MySQL本身的一些限制，可以参考：Restrictions on Stored Programs。下面记录一些常见的或者我碰到的问题，欢迎补充。 FETCH拿不到任何记录如果定义的变量名和SELECT的字段名一样，那么可能出现问题，这是MySQL的一个bug，解决方案是不要使变量名和字段名一样。 ERROR：Cursor declaration after handler declarationDECLARATION定义顺序错误，类似的还有Variable or condition declaration after cursor or handler declaration错误 参考资料 https://dev.mysql.com/doc/refman/5.7/en/create-procedure.html http://chuiliu.github.io/2016/02/28/mysql%E7%9A%84definer%E5%92%8Cinvoker/ https://my.oschina.net/u/1424662/blog/485118 https://dev.mysql.com/doc/refman/5.7/en/sql-syntax-compound-statements.html http://www.yiibai.com/mysql/signal-resignal.html http://www.cnblogs.com/langtianya/p/5534222.html https://stackoverflow.com/questions/40661398/mysql-cursor-fetch-null","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/数据库/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"http://yoursite.com/categories/数据库/MySQL/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/数据库/"},{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}]},{"title":"一个查询API引发的对前后端职责的思考","slug":"一个查询API引发的对前后端职责的思考","date":"2017-10-21T12:49:56.000Z","updated":"2017-11-03T04:21:37.254Z","comments":true,"path":"2017/10/21/一个查询API引发的对前后端职责的思考/","link":"","permalink":"http://yoursite.com/2017/10/21/一个查询API引发的对前后端职责的思考/","excerpt":"","text":"问题描述有一个在线博客系统，系统提供了一个API，前端只需要传递参数：(开始日期，结束日期)，然后就会返回一个这样的JSON:{日期1:新文章数量,日期2:新文章数量...}。现在来了一个新的需求：用户需要查看当天，本周，本月，最近半年或者一年新发布的文章的数量。现在需要设计后端API供前端调用，那么这个API应该如何设计呢？ 一开始，我想到了3种可能的方案： 直接使用之前的API，前端根据天，周，月等单位换算成时间区间，去后端查询出每天的新文章数量，然后在前端累加。 设计5个API，然后每个API处理不同的单位。 设计一个API，然后有一个枚举类型的参数表示5种不同的情况。 我的第一感觉是：方案1是最简单的，方案2看起来好像也可以，方案3感觉有点复杂了。我到底应该选择哪一种方案呢，每种方案的利弊是什么呢？ 方案一该方案很简单而且看起来很灵活，后端提供一个API，既可以用来获取每天新文章具体数目，又可以用来计算该区间内新总和，那么该方案有什么问题吗？ 我觉得这个方案最大的问题就是暴露了领域知识在前端，这里体现出来的就是前端人员需要计算本周的区间，本月的区间，本年的时间区间。当然这个知识很简单，前端人员肯定都知道怎么换算。但是这确实不应该由前端来处理，为什么呢？ 我个人觉得前端人员的职责主要就是单纯的调用后端的API，然后将数据展示出来。前端人员只需要知道哪些API是来干什么的以及调用的顺序即可。 单位的转换确实应该由后端完成。单位的概念也属于领域的知识，本例子中的年月日比较简单，但如果是(点，刻，字)这种时间单位呢？后端处理数据，数据的单位转换就应当交给后端完成。 举个详细的例子来说明由后端处理的好处：查询2017年9月的新文章数量。如果后端来做2017年9月的查询，那么就有这几种很好的实现： 换算成区间，然后使用之前API的代码查询并对结果求和，之后将结果缓存起来。 后端可以基于时间列创建日期列（如果数据库是MySQL可以使用Virtual columns），然后在日期列上创建索引。甚至查询结果也可以缓存起来。 方案二和方案三方案二和方案三都没有方案一的问题。之所以现在要将这两个放在一起说，是因为这两个的关系有点类似于面向对象设计里面的FlagArgument 问题。其建议不要提供一个唯一的API，然后通过额外的参数表示不同的行为，而是推荐提供多个表示不同行为的API。 Martin Fowler讨论FlagArgument时使用了下面的例子： // 1class Concert... public Booking book (Customer aCustomer, boolean isPremium) &#123;...&#125;//2class Concert... public Booking regularBook(Customer aCustomer) &#123;...&#125; public Booking premiumBook(Customer aCustomer) &#123;...&#125; 他给出了不要使用FlagArgument的主要原因： My reasoning here is that the separate methods communicate more clearly what my intention is when I make the call. Instead of having to remember the meaning of the flag variable when I see book(martin, false) I can easily read regularBook(martin). 从可读性和可维护性说起，假如regularBook的处理逻辑需要修改，那么第二种方式可以更好的定位到所有使用了reqularBook逻辑的地方，第一种方式则比较麻烦。但是这种情况并不是绝对的，在编程语言中的关键字参数或者枚举就可以绕过这个问题: 比如在Python中，我们可以使用关键字参数，调用方式大概如下ins.book(customer,isPremium=True) 使用或者使用枚举:ins.book(customer,PriceType.Premium) 现在讨论其扩展性，假如新增了一种价格类型，第一种方式需要将isPremium变成一个可以表示3种情况的枚举，而第二种方式则需要增加一个API，如果情况很多，那么第二种方式将会有大量的API产生。大量API主要会带来什么问题？我觉得主要看调用该API的人是谁，如果是后端自己用的API那么没什么问题，但是如果要给前端调用就有问题了：假如前端不关心价格类型。 举个例子：如果是第一种方式的API，后端需要告诉告诉前端：Hi Jay，我写了一个预定的API，到时候你传递用户编号和价格类型过来给我就可以了。如果是第二种方式则是：Hi Jay，我写了一系列用于预定的API，所有API你都需要传递用户编号过来，如果是XX价格类型，你就调用XXX API，如果是YY价格类型，你就调用YYY API，如果是…。 如果前端又需要关心价格类型，那么仍然可以采用第一种方式，因为访问后端的API时可以提供命名参数，如：book?price_type=premium。 因此在博客系统案例中，我最终选择了方案三，设计的API如下： GET /recent?range=1mrange值范围：1d 1w 1m 6m 1y... 参考资料https://softwareengineering.stackexchange.com/questions/359452/how-do-i-design-a-backend-api-with-a-different-query-range https://softwareengineering.stackexchange.com/questions/147977/is-it-wrong-to-use-a-boolean-parameter-to-determine-behavior https://martinfowler.com/bliki/FlagArgument.html","categories":[{"name":"软件工程","slug":"软件工程","permalink":"http://yoursite.com/categories/软件工程/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"http://yoursite.com/tags/软件工程/"},{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"},{"name":"前后端分离","slug":"前后端分离","permalink":"http://yoursite.com/tags/前后端分离/"}]}]}